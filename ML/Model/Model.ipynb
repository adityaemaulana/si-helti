{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from gensim.models import Word2Vec, WordEmbeddingSimilarityIndex\n",
    "from gensim.similarities import SoftCosineSimilarity, SparseTermSimilarityMatrix\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"idwiki_word2vec_200.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "with open('dataset.json') as json_file:\n",
    "    dataset = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_remover = StopWordRemoverFactory().create_stop_word_remover()\n",
    "stemmer = StemmerFactory().create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(document):\n",
    "    document = sw_remover.remove(document)\n",
    "    document_stem = stemmer.stem(document).split(\" \")\n",
    "    document_token = [w for w in document_stem if w.isalpha()]\n",
    "    return document_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_decease(input_document, docsim_index, dictionary):\n",
    "    query = preprocess(input_document)\n",
    "    sims = docsim_index[dictionary.doc2bow(query)]\n",
    "    predict_result = sims[0]\n",
    "    \n",
    "    data = dataset[predict_result[0]]\n",
    "    weight = predict_result[1]\n",
    "    \n",
    "    result_dict = {'data' : data, 'weight': weight}\n",
    "                   \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Aku ingin mencoba makanan yang sedang dihidangkan mama\"\n",
    "s = sw_remover.remove(s)\n",
    "tokens = stemmer.stem(s).split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_similarity(s1, s2, model):\n",
    "    \n",
    "    s1 = sw_remover.remove(s1)\n",
    "    tokens1 = stemmer.stem(s1).split(\" \")\n",
    "\n",
    "    s2 = sw_remover.remove(s2)\n",
    "    tokens2 = stemmer.stem(s2).split(\" \")\n",
    "\n",
    "    tokens1 = [token for token in tokens1 if token in model.wv]\n",
    "    tokens2 = [token for token in tokens2 if token in model.wv]\n",
    "    \n",
    "#     print(tokens1)\n",
    "#     print(tokens2)\n",
    "#     print(\"------\")\n",
    "    \n",
    "    if len(tokens1) == 0 or len(tokens2) == 0:\n",
    "        return 0\n",
    "\n",
    "    tokfreqs1 = Counter(tokens1)\n",
    "    tokfreqs2 = Counter(tokens2)\n",
    "    \n",
    "#     print(\"Document 1\\n\")\n",
    "#     print(tokfreqs1)\n",
    "#     print(\"Document 2\\n\")\n",
    "#     print(tokfreqs2)\n",
    "    \n",
    "    weights1 = [model.wv[token] / model.wv.vocab[token].count for token in tokfreqs1]\n",
    "    weights2 = [model.wv[token] / model.wv.vocab[token].count for token in tokfreqs2]\n",
    "        \n",
    "    embedding1 = np.average([model.wv[token] for token in tokfreqs1], axis=0, weights=weights1).reshape(1, -1)\n",
    "    embedding2 = np.average([model.wv[token] for token in tokfreqs2], axis=0, weights=weights2).reshape(1, -1)\n",
    "    \n",
    "    sim = cosine_similarity(embedding1, embedding2)[0][0]\n",
    "        \n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03939446822973372\n",
      "0.0266333867430677\n"
     ]
    }
   ],
   "source": [
    "with open('doc1.txt', 'r') as file:\n",
    "    doc1 = file.read().replace('\\n', '')\n",
    "\n",
    "with open('doc2.txt', 'r') as file:\n",
    "    doc2 = file.read().replace('\\n', '')\n",
    "    \n",
    "with open('doc3.txt', 'r') as file:\n",
    "    doc3 = file.read().replace('\\n', '')\n",
    "    \n",
    "print(doc_similarity(doc1, doc2, model))\n",
    "# print(doc_similarity(doc2, doc3, model))\n",
    "print(doc_similarity(doc1, doc3, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for docs in dataset:\n",
    "    str_gejala = \"\"\n",
    "    for gejala in docs['gejala']:\n",
    "        str_gejala += \".\" + gejala\n",
    "    \n",
    "    predict = []\n",
    "    max_sim = 0\n",
    "    for d in dataset:\n",
    "        str_gejala_test = \"\"\n",
    "        for gejala in d['gejala']:\n",
    "            str_gejala_test += \".\" + gejala\n",
    "        \n",
    "        sim = doc_similarity(str_gejala, str_gejala_test, model)\n",
    "        if(max_sim < sim) :\n",
    "            max_sim = sim\n",
    "            predict.append(d['judul'])\n",
    "\n",
    "cor = 0\n",
    "for i in range(len(predict)):\n",
    "    if(predict[i] == dataset[i]['judul']):\n",
    "        cor += 1\n",
    "\n",
    "print(cor / len(predict) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Term Similarity Index from Word2Vec model\n",
    "\n",
    "termsim_index = WordEmbeddingSimilarityIndex(model.wv)\n",
    "\n",
    "# Create Corpus List\n",
    "corpus_list = []\n",
    "for data in dataset:\n",
    "    docs = \"\"\n",
    "    for sentence in data['gejala']:\n",
    "        docs += \" \" + sentence\n",
    "    corpus_list.append(docs)\n",
    "\n",
    "# Create token list for all document corpus\n",
    "corpus_list_token = [preprocess(doc) for doc in corpus_list]\n",
    "# \n",
    "dictionary = Dictionary(corpus_list_token)\n",
    "bow_corpus = [dictionary.doc2bow(document) for document in corpus_list_token]\n",
    "\n",
    "# Create Term similarity matrix\n",
    "similarity_matrix = SparseTermSimilarityMatrix(termsim_index, dictionary)\n",
    "\n",
    "# Compute Soft Cosine Similarity\n",
    "docsim_index = SoftCosineSimilarity(bow_corpus, similarity_matrix, num_best=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aemaulana/anaconda3/lib/python3.6/site-packages/gensim/similarities/termsim.py:357: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  Y = np.multiply(Y, 1 / np.sqrt(Y_norm))\n",
      "/home/aemaulana/anaconda3/lib/python3.6/site-packages/gensim/similarities/termsim.py:357: RuntimeWarning: invalid value encountered in multiply\n",
      "  Y = np.multiply(Y, 1 / np.sqrt(Y_norm))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'data': {'judul': 'Sepsis Neonatorum',\n",
       "  'definisi': 'Sepsis Neonatorum adalah suatu infeksi bakteri berat yang menyebar ke seluruh tubuh bayi baru lahir.\\nSepsis terjadi pada kurang dari 1% bayi baru lahir tetapi merupakan penyebab dari 30% kematian pada bayi baru lahir.\\nInfeksi bakteri 5 kali lebih sering terjadi pada bayi baru lahir yang berat badannya kurang dari 2,75 kg dan 2 kali lebih sering menyerang bayi laki-laki.\\nPada lebih dari 50% kasus, sepsis mulai timbul dalam waktu 6 jam setelah bayi lahir, tetapi kebanyakan muncul dalamw aktu 72 jam setelah lahir.\\nSepsis yang baru timbul dalam waktu 4 hari atau lebih kemungkinan disebabkan oleh infeksi nasokomial (infeksi yang didapat di rumah sakit).\\n',\n",
       "  'gejala': ['Gangguan pernafasan',\n",
       "   'Kejang',\n",
       "   'Jaundice (sakit kuning)',\n",
       "   'Muntah',\n",
       "   'Diare',\n",
       "   'Perut kembung.']},\n",
       " 'weight': 0.8293212056159973}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Diare (10 hingga 12 kali per hari) Diare disertai darah.Kram pada perut.Buang air besar yang kental.Gas dalam perut.Gejala yang umum seperti demam, sakit punggung, dan lelah.\" \n",
    "predict = predict_decease(text, docsim_index, dictionary)\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['diare', 'hari', 'diare', 'serta', 'darah']\n",
      "['ganggu', 'nafas']\n",
      "------\n",
      "['kram', 'perut']\n",
      "['kejang']\n",
      "------\n",
      "['buang', 'air', 'kental']\n",
      "['jaundice', 'sakit', 'kuning']\n",
      "------\n",
      "['gas', 'perut']\n",
      "['muntah']\n",
      "------\n",
      "['gejala', 'umum', 'demam', 'sakit', 'punggung', 'lelah']\n",
      "['diare']\n",
      "------\n",
      "[]\n",
      "['perut', 'kembung']\n",
      "------\n",
      "[-0.06878397, 0.09454537, -0.04392944, 0.004184945, -0.10668429, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Gangguan pernafasan',\n",
       " 'Kejang',\n",
       " 'Jaundice (sakit kuning)',\n",
       " 'Muntah',\n",
       " 'Diare',\n",
       " 'Perut kembung.']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_end = re.compile(r'''[.!?]['\"]?\\s{1,2}(?=)''')\n",
    "input_sentences = re.split(r\"\\.|\\?|\\!\",text)\n",
    "\n",
    "print(docs_similarity(input_sentences, [sentence for sentence in predict['data']['gejala']], model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
